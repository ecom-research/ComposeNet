# Copyright 2019 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Models for Text and Image Composition."""
from collections import OrderedDict

import numpy as np
import torch
import torchvision
import torch.nn.functional as F
import text_model
import re
import torch_functions
import torch.nn as nn
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch.nn.utils.weight_norm import weight_norm
import torch.backends.cudnn as cudnn
from torch.nn.utils.clip_grad import clip_grad_norm
from bert_serving.client import BertClient
from gensim.models import KeyedVectors

# wv = KeyedVectors.load('../tirg-with-scan/wordvectors-300.kv', mmap='r')
"""this is test string 222"""
"""sss"""
bc = BertClient()


class ConCatModule(torch.nn.Module):

    def __init__(self):
        super(ConCatModule, self).__init__()

    def forward(self, x):
        x = torch.cat(x, 1)

        return x

class ConcatWithLinearModuleUseConv1(torch.nn.Module):

    def __init__(self):
        super(ConcatWithLinearModuleUseConv1, self).__init__()
        self.conv1 = torch.nn.Conv1d(2, 1, kernel_size=1)
        self.image_l1 = torch.nn.Sequential(
            torch.nn.BatchNorm1d(512),
            torch.nn.Linear(512, 512)
        )
        self.text_l1 = torch.nn.Sequential(
            torch.nn.BatchNorm1d(768),
            torch.nn.Linear(768, 512)
        )

        self.weight_img = torch.nn.Parameter(torch.FloatTensor(512, 512))
        self.weight_text = torch.nn.Parameter(torch.FloatTensor(512, 512))

        # torch.nn.init.xavier_uniform_(self.weight_img)
        # torch.nn.init.xavier_uniform_(self.weight_text)

    def forward(self, x):
        image_f = x[0]
        text_f = x[1]

        # first_concat = torch.cat([image_f, text_f], 1).unsqueeze(1)

        image_f = self.image_l1(image_f)
        text_f = self.text_l1(text_f)

        second_concat = torch.cat([image_f, text_f], 1).unsqueeze(1)

        # image dropout
        image_f = F.dropout(image_f, p=0.5)

        # relu first
        image_f = F.relu(image_f)
        text_f = F.relu(text_f)

        # linear second
        image_f = F.linear(image_f, self.weight_img)
        text_f = F.linear(text_f, self.weight_text)

        third_concat = torch.cat([image_f, text_f], 1).unsqueeze(1)

        concatenated = torch.cat([second_concat, third_concat], 1)

        return self.conv1(concatenated).squeeze(1)

class ImgTextCompositionBase(torch.nn.Module):
    """Base class for image + text composition."""

    def __init__(self):
        super(ImgTextCompositionBase, self).__init__()
        self.normalization_layer = torch_functions.NormalizationLayer(
            normalize_scale=4.0, learn_scale=True)
        self.soft_triplet_loss = torch_functions.TripletLoss()

    def extract_img_feature(self, imgs):
        raise NotImplementedError

    def extract_text_feature(self, texts):
        raise NotImplementedError

    def compose_img_text(self, imgs, texts):
        raise NotImplementedError

    def compute_loss(self,
                     imgs_query,
                     modification_texts,
                     imgs_target,
                     soft_triplet_loss=True):
        mod_img1 = self.compose_img_text(imgs_query, modification_texts)  # ids
        mod_img1 = self.normalization_layer(mod_img1)
        img2 = self.extract_img_feature(imgs_target)
        img2 = self.normalization_layer(img2)
        assert (mod_img1.shape[0] == img2.shape[0] and
                mod_img1.shape[1] == img2.shape[1])

        if soft_triplet_loss:
            return self.compute_soft_triplet_loss_(mod_img1, img2)
        else:
            return self.compute_batch_based_classification_loss_(mod_img1, img2)

    def compute_loss_with_extra_data(self,
                                     imgs_query,
                                     modification_texts,
                                     imgs_target,
                                     nouns,
                                     soft_triplet_loss=True):
        mod_img1_non_norm, img1, repr_to_compare_with_source, repr_to_compare_with_mods, text_features = self.compose_img_text_with_extra_data(
            imgs_query, modification_texts, nouns)
        mod_img1 = self.normalization_layer(mod_img1_non_norm)
        img2_non_norm = self.extract_img_feature(imgs_target)
        img2 = self.normalization_layer(img2_non_norm)  # ????????????

        assert (mod_img1.shape[0] == img2.shape[0] and
                mod_img1.shape[1] == img2.shape[1])
        if soft_triplet_loss:
            return self.compute_soft_triplet_loss_(mod_img1,
                                                   img2), img2_non_norm, mod_img1_non_norm, img1, repr_to_compare_with_source, repr_to_compare_with_mods, text_features
        else:
            return self.compute_batch_based_classification_loss_(mod_img1, img2)

    def compute_soft_triplet_loss_(self, mod_img1, img2):
        triplets = []
        labels = range(mod_img1.shape[0]) + range(img2.shape[0])
        for i in range(len(labels)):
            triplets_i = []
            for j in range(len(labels)):
                if labels[i] == labels[j] and i != j:
                    for k in range(len(labels)):
                        if labels[i] != labels[k]:
                            triplets_i.append([i, j, k])
            np.random.shuffle(triplets_i)
            triplets += triplets_i[:3]
        assert (triplets and len(triplets) < 2000)
        return self.soft_triplet_loss(torch.cat([mod_img1, img2]), triplets)

    def compute_batch_based_classification_loss_(self, mod_img1, img2):
        x = torch.mm(mod_img1, img2.transpose(0, 1))
        labels = torch.tensor(range(x.shape[0])).long()
        labels = torch.autograd.Variable(labels).cuda()
        return F.cross_entropy(x, labels)

class ImgEncoderTextEncoderBase(ImgTextCompositionBase):
    """Base class for image and text encoder."""

    def __init__(self, texts, embed_dim, learn_on_regions):
        super(ImgEncoderTextEncoderBase, self).__init__()
        self.learn_on_regions = learn_on_regions

        # text model
        #         self.text_model = text_model.TextLSTMModel(
        #             texts_to_build_vocab=texts,
        #             word_embed_dim=embed_dim,
        #             lstm_hidden_dim=embed_dim)

        if self.learn_on_regions:
            # overwrite img_model
            print("Using just linear layer for img_model...")
            self.img_model = torch.nn.Sequential(torch.nn.Linear(2048, 512))
            return

        # img model
        img_model = torchvision.models.resnet18(pretrained=True)

        #         for param in img_model.parameters():
        #             param.requires_grad = False

        class GlobalAvgPool2d(torch.nn.Module):

            def forward(self, x):
                return F.adaptive_avg_pool2d(x, (1, 1))

        img_model.avgpool = GlobalAvgPool2d()  # change shape?
        img_model.fc = torch.nn.Sequential(torch.nn.Linear(512, 512))
        self.img_model = img_model

    def extract_img_feature(self, imgs):
        return self.img_model(imgs)

    def extract_text_feature(self, texts):
        return self.text_model(texts)

class SimpleModelImageOnly(ImgEncoderTextEncoderBase):

    def compose_img_text(self, imgs, texts):
        return self.extract_img_feature(imgs)

class SimpleModelTextOnly(ImgEncoderTextEncoderBase):
    def compose_img_text(self, imgs, texts):
        return self.extract_text_feature(texts)

class Concat(ImgEncoderTextEncoderBase):
    """Concatenation model."""

    def __init__(self, texts, embed_dim, learn_on_regions):
        super(Concat, self).__init__(texts, embed_dim)

        # composer
        class Composer(torch.nn.Module):
            """Inner composer class."""

            def __init__(self):
                super(Composer, self).__init__()
                self.m = torch.nn.Sequential(
                    torch.nn.BatchNorm1d(2 * embed_dim),
                    torch.nn.ReLU(),
                    torch.nn.Linear(2 * embed_dim, 2 * embed_dim),
                    torch.nn.BatchNorm1d(2 * embed_dim),
                    torch.nn.ReLU(),
                    torch.nn.Dropout(0.1),
                    torch.nn.Linear(2 * embed_dim, embed_dim))

            def forward(self, x):
                f = torch.cat(x, dim=1)
                f = self.m(f)
                return f

        self.composer = Composer()

    def compose_img_text(self, imgs, texts):
        img_features = self.extract_img_feature(imgs)
        text_features = self.extract_text_feature(texts)
        return self.compose_img_text_features(img_features, text_features)

    def compose_img_text_features(self, img_features, text_features):
        return self.composer((img_features, text_features))

class TIRG(ImgEncoderTextEncoderBase):
    """The TIGR model.

    The method is described in
    Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays.
    "Composing Text and Image for Image Retrieval - An Empirical Odyssey"
    CVPR 2019. arXiv:1812.07119
    """

    def __init__(self, texts, embed_dim, learn_on_regions):
        super(TIRG, self).__init__(texts, embed_dim, learn_on_regions)

        self.a = torch.nn.Parameter(torch.tensor([1.0, 10.0, 1.0, 1.0]))
        self.gated_feature_composer = torch.nn.Sequential(
            ConCatModule(),
            torch.nn.BatchNorm1d(2 * embed_dim),
            # torch.nn.Dropout(p=0.2),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, embed_dim))
        self.res_info_composer = torch.nn.Sequential(
            ConCatModule(),
            torch.nn.BatchNorm1d(2 * embed_dim),
            # torch.nn.Dropout(p=0.2),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, 2 * embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, embed_dim))

    def compose_img_text(self, imgs, texts):
        img_features = self.extract_img_feature(imgs)

        text_features = self.extract_text_feature(texts)
        return self.compose_img_text_features(img_features, text_features)

    def compose_img_text_features(self, img_features, text_features):
        f1 = self.gated_feature_composer((img_features, text_features))
        f2 = self.res_info_composer((img_features, text_features))
        f = torch.sigmoid(f1) * img_features * self.a[0] + f2 * self.a[1]
        return f
# class TIRGEvolved(ImgEncoderTextEncoderBase):
#     """The TIGR model.

#     The method is described in
#     Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays.
#     "Composing Text and Image for Image Retrieval - An Empirical Odyssey"
#     CVPR 2019. arXiv:1812.07119
#     """

#     def __init__(self, texts, embed_dim, learn_on_regions):
#         super(TIRGEvolved, self).__init__(texts, embed_dim, learn_on_regions)
#         # self.conv1 = torch.nn.Conv1d(2, 1, kernel_size=1)
#         self.encoder_1 = torch.nn.Sequential(ConcatWithLinearModule())
#         self.encoder_2 = torch.nn.Sequential(torch.nn.BatchNorm1d(512 + 512),
#                                              torch.nn.ReLU(),
#                                              torch.nn.Linear(512 + 512, 512 + 512))
#         self.encoder_3 = torch.nn.Sequential(torch.nn.BatchNorm1d(512 + 512),
#                                              torch.nn.ReLU(),
#                                              torch.nn.Linear(512 + 512, embed_dim))
#         self.encoder_4 = torch.nn.Sequential(torch.nn.BatchNorm1d(512),
#                                              torch.nn.ReLU(),
#                                              torch.nn.Linear(embed_dim, embed_dim))


#     def compose_img_text_with_extra_data(self, imgs, texts, extra_data):
#         img_features = self.extract_img_feature(imgs)

#         text_features = bc.encode([adj + " " + noun for adj, noun in zip(texts, extra_data)])
#         text_features = torch.from_numpy(text_features).cuda()

#         return self.compose_img_text_features(img_features, text_features)

#     def compose_img_text_features(self, img_features, text_features):
#         concat_with_linear = self.encoder_1((img_features, text_features))
#         encoded_2 = self.encoder_2(concat_with_linear)
#         # encoded_2_with_dropout = F.dropout(encoded_2, p=0.2)
#         encoded_3 = self.encoder_3(encoded_2.add(concat_with_linear))

# #         concatenated = torch.cat([concat_with_linear.unsqueeze(1),
# #                                   encoded_2.unsqueeze(1),
# #                                   encoded_3.unsqueeze(1)],
# #                                  1)
#         return self.encoder_4(encoded_3), img_features
#         # return self.conv1(concatenated).squeeze(1), img_features
class ConcatWithLinearModule(torch.nn.Module):

    def __init__(self):
        super(ConcatWithLinearModule, self).__init__()
        self.bert_features = torch.nn.Sequential(
            torch.nn.BatchNorm1d(768),
            torch.nn.Linear(768, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 512)
        )
        self.image_features = torch.nn.Sequential(
            torch.nn.BatchNorm1d(512),
            torch.nn.Linear(512, 512),
            torch.nn.Dropout(p=0.7),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 512),
        )

    def forward(self, x):
        x1 = self.image_features(x[0])
        x2 = self.bert_features(x[1])
        concat_x = torch.cat([x1, x2], 1)

        return concat_x

class TIRGEvolved(ImgEncoderTextEncoderBase):
    """The TIGR model.

    The method is described in
    Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays.
    "Composing Text and Image for Image Retrieval - An Empirical Odyssey"
    CVPR 2019. arXiv:1812.07119
    """

    def __init__(self, texts, embed_dim, learn_on_regions):
        super(TIRGEvolved, self).__init__(texts, embed_dim, learn_on_regions)
        self.a = torch.nn.Parameter(torch.tensor([1.0, 10.0, 1.0, 1.0]))
        self.gated_feature_composer = torch.nn.Sequential(
            ConcatWithLinearModule(),
            torch.nn.BatchNorm1d(2 * embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, embed_dim)
        )
        self.res_info_composer = torch.nn.Sequential(
            ConcatWithLinearModule(),
            torch.nn.BatchNorm1d(2 * embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, 2 * embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2 * embed_dim, embed_dim))
        self.decoder = torch.nn.Sequential(
            torch.nn.BatchNorm1d(embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embed_dim, embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embed_dim, embed_dim)
        )
        self.txtdecoder = torch.nn.Sequential(
            torch.nn.BatchNorm1d(embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embed_dim, 768),
            torch.nn.ReLU(),
            torch.nn.Linear(768, 768)
        )
        self.encoder = torch.nn.Sequential(
            ConcatWithLinearModule(),
            torch.nn.BatchNorm1d(2*embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(2*embed_dim, embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embed_dim, embed_dim)
        )

    def compose_img_text_with_extra_data(self, imgs, texts, extra_data):
        img_features = self.extract_img_feature(imgs)

        text_features = bc.encode([adj + " " + noun for adj, noun in zip(texts, extra_data)])
        text_features = torch.from_numpy(text_features).cuda()

        return self.compose_img_text_features(img_features, text_features)

    def compose_img_text_features(self, img_features, text_features):
        f1 = self.gated_feature_composer((img_features, text_features))
        f2 = self.res_info_composer((img_features, text_features))
        repres = self.encoder((img_features, text_features))
        Comb_AE_TIRG = F.sigmoid(f1) * img_features * self.a[0] + f2 * self.a[1] + repres * self.a[2]
        repr_to_compare_with_source = self.decoder(repres)
        repr_to_compare_with_mods = self.txtdecoder(repres)
        return Comb_AE_TIRG, img_features, repr_to_compare_with_source, repr_to_compare_with_mods, text_features

# class RotateAE(ImgEncoderTextEncoderBase):
#     """The Rotate_AE model.
#
#     The method is described in
#     Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays.
#     "Composing Text and Image for Image Retrieval - An Empirical Odyssey"
#     CVPR 2019. arXiv:1812.07119
#     """
#
#     def __init__(self, texts, embed_dim, learn_on_regions):
#         super(RotateAE, self).__init__(texts, embed_dim, learn_on_regions)
#         self.a = torch.nn.Parameter(torch.tensor([1.0, 10.0, 1.0, 1.0]))
#         self.gated_feature_composer = torch.nn.Sequential(
#             ConcatWithLinearModule(),
#             torch.nn.BatchNorm1d(2 * embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(2 * embed_dim, embed_dim)
#         )
#         self.res_info_composer = torch.nn.Sequential(
#             ConcatWithLinearModule(),
#             torch.nn.BatchNorm1d(2 * embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(2 * embed_dim, 2 * embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(2 * embed_dim, embed_dim))
#         self.decoder = torch.nn.Sequential(
#             torch.nn.BatchNorm1d(embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(embed_dim, embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(embed_dim, embed_dim)
#         )
#         self.txtdecoder = torch.nn.Sequential(
#             torch.nn.BatchNorm1d(embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(embed_dim, 768),
#             torch.nn.ReLU(),
#             torch.nn.Linear(768, 768)
#         )
#         self.encoder = torch.nn.Sequential(
#             ConcatWithLinearModule(),
#             torch.nn.BatchNorm1d(2*embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(2*embed_dim, embed_dim),
#             torch.nn.ReLU(),
#             torch.nn.Linear(embed_dim, embed_dim)
#         )
#
#     def compose_img_text_with_extra_data(self, imgs, texts, extra_data):
#         img_features = self.extract_img_feature(imgs)
#
#         text_features = bc.encode([adj + " " + noun for adj, noun in zip(texts, extra_data)])
#         text_features = torch.from_numpy(text_features).cuda()
#
#         return self.compose_img_text_features(img_features, text_features)
#
#     def compose_img_text_features(self, img_features, text_features):
#         #         f1 = self.gated_feature_composer((img_features, text_features))
#         #         f2 = self.res_info_composer((img_features, text_features))
#         #         f = F.sigmoid(f1) * img_features * self.a[0] + f2 * self.a[1]
#         repres = self.encoder((img_features, text_features))
#         repr_to_compare_with_source = self.decoder(repres)
#         repr_to_compare_with_mods = self.txtdecoder(repres)
#         return repres, img_features, repr_to_compare_with_source, repr_to_compare_with_mods, text_features

#         self.encoder = torch.nn.Sequential(
#             ConcatWithLinearModule(),
#             torch.nn.BatchNorm1d(512 + 512),
#             torch.nn.ReLU(),
#             torch.nn.Linear(512 + 512, 512 + 512),
#             torch.nn.BatchNorm1d(512 + 512),
#             torch.nn.ReLU(),
#             # torch.nn.Dropout(0.1),
#             torch.nn.Linear(512 + 512, embed_dim)
#         ) # mitstates_tirg_evolved_resnet18_repeat14-21-36_no_DO

def l1norm(X, dim, eps=1e-8):
    """L1-normalize columns of X
    """
    norm = torch.abs(X).sum(dim=dim, keepdim=True) + eps
    X = torch.div(X, norm)
    return X


def l2norm(X, dim, eps=1e-8):
    """L2-normalize columns of X
    """
    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps
    X = torch.div(X, norm)
    return X


def EncoderImage(img_dim, embed_size, precomp_enc_type='basic',
                 no_imgnorm=False):
    """A wrapper to image encoders. Chooses between an different encoders
    that uses precomputed image features.
    """
    if precomp_enc_type == 'basic':
        img_enc = EncoderImagePrecomp(
            img_dim, embed_size, no_imgnorm)
    elif precomp_enc_type == 'weight_norm':
        img_enc = EncoderImageWeightNormPrecomp(
            img_dim, embed_size, no_imgnorm)
    else:
        raise ValueError("Unknown precomp_enc_type: {}".format(precomp_enc_type))

    return img_enc


class EncoderImagePrecomp(nn.Module):

    def __init__(self, img_dim, embed_size, no_imgnorm=False):
        super(EncoderImagePrecomp, self).__init__()
        self.embed_size = embed_size
        self.no_imgnorm = no_imgnorm
        self.fc = nn.Linear(img_dim, embed_size)

        self.init_weights()

    def init_weights(self):
        """Xavier initialization for the fully connected layer
        """
        r = np.sqrt(6.) / np.sqrt(self.fc.in_features +
                                  self.fc.out_features)
        self.fc.weight.data.uniform_(-r, r)
        self.fc.bias.data.fill_(0)

    def forward(self, images):
        """Extract image feature vectors."""
        # assuming that the precomputed features are already l2-normalized

        features = self.fc(images)

        # normalize in the joint embedding space
        if not self.no_imgnorm:
            features = l2norm(features, dim=-1)

        return features

    def load_state_dict(self, state_dict):
        """Copies parameters. overwritting the default one to
        accept state_dict from Full model
        """
        own_state = self.state_dict()
        new_state = OrderedDict()
        for name, param in state_dict.items():
            if name in own_state:
                new_state[name] = param

        super(EncoderImagePrecomp, self).load_state_dict(new_state)


class EncoderImageWeightNormPrecomp(nn.Module):

    def __init__(self, img_dim, embed_size, no_imgnorm=False):
        super(EncoderImageWeightNormPrecomp, self).__init__()
        self.embed_size = embed_size
        self.no_imgnorm = no_imgnorm
        self.fc = weight_norm(nn.Linear(img_dim, embed_size), dim=None)

    def forward(self, images):
        """Extract image feature vectors."""
        # assuming that the precomputed features are already l2-normalized

        features = self.fc(images)

        # normalize in the joint embedding space
        if not self.no_imgnorm:
            features = l2norm(features, dim=-1)

        return features

    def load_state_dict(self, state_dict):
        """Copies parameters. overwritting the default one to
        accept state_dict from Full model
        """
        own_state = self.state_dict()
        new_state = OrderedDict()
        for name, param in state_dict.items():
            if name in own_state:
                new_state[name] = param

        super(EncoderImageWeightNormPrecomp, self).load_state_dict(new_state)